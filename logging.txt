    # print(x_train.shape) # (17648, 30)
    # print(t_train.shape) # (17648,)
    # print(x_val.shape) # (4412, 30)
    # print(t_val.shape) # (4412,)
    # print(Counter(t_train))
    # print(Counter(t_val))

"""
data as it is : 
    logistic regression:
        solver: sag, fit-intercept=True, max-iterations=10000
        f1-score=88%

    random forest:
        max-depth,n-estimators : {[9,100],[9,50], [9,20], [6,50], [6,20], [5,100]}
        f1-score=88%

    xgboost:
        max-depth: 3, lr=0.2, n-estimator:100
        f1-score=77%
    
    light-boost:
        n-estimator:500, lr=0.05, max-depth:3
        f1-score: 79%

    cat-boost:
        depth: 3, iterations: 100, lr=0.1
        f1-score: 88%
------------------------------------------------------------------------------------------------
Over sampling Minority:
    Logistic-regression: 
        over-factor:80
        f1-score: 89%

    random-forest:
        over-factor:80
        f1-score:89%

    xgboost:
        over-factor:80
        f1-score:88%

    light-boost:
        over-factor:80
        f1-score:85%

    cat-boost:
        over-factor:80
        f1-score:80%
------------------------------------------------------------------------------------------------

Under sampling Majority:
    Logistic-regression: 
        over-factor:5
        f1-score: 80%

    random-forest:
        over-factor:80
        f1-score:88
    
    xgboost:
        over-factor:40
        f1-score:88%

    light-boost:
        over-factor:100
        f1-score:84%

    cat-boost:
        over-factor:40
        f1-score:80%
------------------------------------------------------------------------------------------------
Under sampling Majority:
    Logistic-regression: 
        over-factor:5, under-factor:5
        f1-score: 89%

    random-forest:
        over-factor:10, under-factor:10
        f1-score:89%

    xgboost:
        over-factor:5, under-factor: 5
        f1-score:89%

    light-boost:
        over-factor:20, under-factor:20
        f1-score:83%

    cat-boost:
        over-factor:10, under-factor:10
        f1-score:89%


"""

"""
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56812
           1       0.68      0.84      0.75        96

    accuracy                           1.00     56908
   macro avg       0.84      0.92      0.88     56908
weighted avg       1.00      1.00      1.00     56908

"""
