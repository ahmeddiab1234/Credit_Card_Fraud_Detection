    # print(x_train.shape) # (17648, 30)
    # print(t_train.shape) # (17648,)
    # print(x_val.shape) # (4412, 30)
    # print(t_val.shape) # (4412,)
    # print(Counter(t_train))
    # print(Counter(t_val))

"""
data as it is : 
    logistic regression:
        solver: sag, fit-intercept=True, max-iterations=10000
        f1-score=88%

    random forest:
        max-depth,n-estimators : {[9,100],[9,50], [9,20], [6,50], [6,20], [5,100]}
        f1-score=88%

    xgboost:
        max-depth: 3, lr=0.2, n-estimator:100
        f1-score=77%
    
    light-boost:
        n-estimator:500, lr=0.05, max-depth:3
        f1-score: 79%

    cat-boost:
        depth: 3, iterations: 100, lr=0.1
        f1-score: 88%
------------------------------------------------------------------------------------------------
Over sampling Minority:
    Logistic-regression: 
        over-factor:80
        f1-score: 89%

    random-forest:
        over-factor:80
        f1-score:89%

    xgboost:
        over-factor:80
        f1-score:88%

    light-boost:
        over-factor:80
        f1-score:85%

    cat-boost:
        over-factor:80
        f1-score:80%
------------------------------------------------------------------------------------------------

Under sampling Majority:
    Logistic-regression: 
        over-factor:5
        f1-score: 80%

    random-forest:
        over-factor:80
        f1-score:88
    
    xgboost:
        over-factor:40
        f1-score:88%

    light-boost:
        over-factor:100
        f1-score:84%

    cat-boost:
        over-factor:40
        f1-score:80%
------------------------------------------------------------------------------------------------
Under sampling Majority:
    Logistic-regression: 
        over-factor:5, under-factor:5
        f1-score: 89%

    random-forest:
        over-factor:10, under-factor:10
        f1-score:89%

    xgboost:
        over-factor:5, under-factor: 5
        f1-score:89%

    light-boost:
        over-factor:20, under-factor:20
        f1-score:83%

    cat-boost:
        over-factor:10, under-factor:10
        f1-score:89%


"""

"""

test random forest

              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56812
           1       0.78      0.82      0.80        96

    accuracy                           1.00     56908
   macro avg       0.89      0.91      0.90     56908
weighted avg       1.00      1.00      1.00     56908

"""

"""
val knn oversampling
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     11362
           1       1.00      0.74      0.85       142

    accuracy                           1.00     11504
   macro avg       1.00      0.87      0.92     11504
weighted avg       1.00      1.00      1.00     11504

Time Taken for Preiction 0.4034440517425537
0.15 0.9 0.7676056338028169


val knn data as it is

"""

"""
knn test 
precision    recall  f1-score   support

           0       1.00      1.00      1.00     56812
           1       0.76      0.60      0.67        96

    accuracy                           1.00     56908
   macro avg       0.88      0.80      0.84     56908
weighted avg       1.00      1.00      1.00     56908

2.4936275482177734
"""

""""
test shape: (56960, 31)
""""

